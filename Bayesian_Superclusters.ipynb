{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "561be402",
   "metadata": {},
   "source": [
    "# Bayesian MCMC Analysis of Magnetic Fields in Galaxy Superclusters\n",
    "\n",
    "This notebook demonstrates a **Bayesian statistical analysis** of magnetic fields in galaxy superclusters.  \n",
  
    "The workflow uses **Rotation Measure (RM) Synthesis** from LOFAR observations, with a focus on:\n",
    "\n",
    "- Selecting real polarized sources from RM catalogs\n",
    "- Inspecting Faraday depth spectra (FDFs) and Stokes Q/U parameters\n",
    "- Cleaning catalogs and handling duplicates\n",
    "- Applying **Bayesian inference with** `emcee` to estimate rotation measures\n",
    "- Visualizing posterior distributions with `corner`\n",
    "\n",
    "---\n",
    "\n",
    "## Why Bayesian?\n",
    "Astrophysical data often contain significant noise and uncertainty.  \n",
    "Using Bayesian methods we can:\n",
    "- Incorporate prior knowledge about expected RM distributions\n",
    "- Sample full posterior distributions with `emcee`\n",
    "- Quantify credible intervals and correlations between parameters\n",
    "- Interpret results visually through posterior plots\n",
    "\n",
    "This notebook demonstrates not only the astrophysical application,  \n",
    "but also practical **Bayesian data analysis skills** relevant for data science and research.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4a3fce",
   "metadata": {},
   "source": [
    "### Step 1: Source Selection from RM-Synthesis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e646ad09",
   "metadata": {},
   "source": [
    "##### Note: I selected 3 superclusters based on declination (for LOFAR) and redshift (z<0.1). These 3 were famous ones, but I already also selected MSCC 175 and MSCC 236 (Santiago-Bautista+2020 catalog).\n",
    "\n",
    "\n",
    "After selecting the LoTSS-DR3 pointings covering the sky area of the supercluster, RM-Synthesis is run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db0b20f",
   "metadata": {},
   "source": [
    "To select the real sources you work on the RMcatalog.csv. RMsynth_json_all.csv contains some additional rows that are useful to have in the main file. Once you filter with your criteria, you can keep only the useful columns for the real sources and have RMcatalog_real_filtered.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dce68b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from astropy.table import Table\n",
    "from astropy.io import fits\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "#To make astropy print all rows in a long table\n",
    "from astropy.table.pprint import conf\n",
    "conf.max_lines = -1\n",
    "conf.max_width = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2f6ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_directory = './'  \n",
    "column_to_check = ' real_leak'  \n",
    "\n",
    "for folder in os.listdir(base_directory):\n",
    "    folder_path = os.path.join(base_directory, folder)\n",
    "    \n",
    "    if os.path.isdir(folder_path):\n",
    "        input_file_1 = os.path.join(folder_path, 'RMcatalog.csv')\n",
    "        input_file_2 = os.path.join(folder_path, 'RMsynth_json_all.csv')\n",
    "        \n",
    "        if os.path.exists(input_file_1):\n",
    "            # Read the input CSV\n",
    "            catalog1_file = input_file_1\n",
    "            catalog2_file = input_file_2\n",
    "\n",
    "            # Read the two catalogs, append rows from catalog2 to catalog1 and fix some formatting\n",
    "            catalog1 = pd.read_csv(catalog1_file)\n",
    "            catalog2 = pd.read_csv(catalog2_file)\n",
    "\n",
    "            column_to_append = catalog2['dFDFth']\n",
    "            catalog1 = pd.concat([catalog1, column_to_append], axis=1)\n",
    "            catalog1[' RMspo'] = catalog1[' RMspo'].astype(float)\n",
    "            catalog1[' poldeg_spo'] = catalog1[' poldeg_spo'].astype(float)\n",
    "\n",
    "            \n",
    "            #Check 'real_leak' after merging and filter rows with 'r'\n",
    "            output_catalog = catalog1[catalog1[' real_leak'] == ' r ']\n",
    "            output_catalog = output_catalog[(output_catalog[' RMspo'] <= 300) & (-300<=output_catalog[' RMspo'])]\n",
    "            output_catalog = output_catalog[output_catalog[' num_peaks'] <=4]\n",
    "           \n",
    "            #For 463/474 these were specific duplicated fake sources from strong polarized source in the field\n",
    "            #output_catalog = output_catalog[(output_catalog[' RMspo'] <9) | (output_catalog[' RMspo']>13)]\n",
    "\n",
    "            #For 278\n",
    "            #output_catalog = output_catalog[(output_catalog[' RMspo'] <-25) | (output_catalog[' RMspo']>-21)]\n",
    "\n",
    "\n",
    "            condition = (-5 < output_catalog[' RMspo']) & (output_catalog[' RMspo'] < 5) & (output_catalog[' poldeg_spo'] < 2.0) | (output_catalog[' poldeg_spo'] > 30.0)\n",
    "            filtered_catalog1 = output_catalog[~condition]\n",
    "\n",
    "\n",
    "            # Specify the columns to keep in the output catalog\n",
    "            selected_columns = ['field', ' isl', ' xpix', ' ypix', ' I_144MHz', ' RMspo',\n",
    "                                ' polint_spo', ' poldeg_spo', ' real_leak',' ra', ' dec',\n",
    "                                ' glon', ' glat', 'dFDFcorMAD', 'dFDFth', 'phiPeakPIfit_rm2',\n",
    "                                'dPhiPeakPIfit_rm2', 'ampPeakPIfit', 'dAmpPeakPIfit',\n",
    "                                'polAngleFit_deg', 'dPolAngleFit_deg']\n",
    "\n",
    "            # Create the output catalog with selected columns\n",
    "            filtered_catalog1 = filtered_catalog1[selected_columns]\n",
    "            filtered_catalog1['SNR_MAD'] = filtered_catalog1['ampPeakPIfit'] / filtered_catalog1['dFDFcorMAD']\n",
    "            filtered_catalog1['SNR_th'] = filtered_catalog1['ampPeakPIfit'] / filtered_catalog1['dFDFth']\n",
    "\n",
    "            #More conservative? Keep?\n",
    "            filtered_catalog1 = filtered_catalog1[filtered_catalog1['SNR_th']>=8]\n",
    "            # Save the output catalog to a CSV file\n",
    "            \n",
    "            output_file = os.path.join(folder_path, 'RMcatalog_real_filtered.csv')\n",
    "            filtered_catalog1.to_csv(output_file, index=False)            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cef6dd1",
   "metadata": {},
   "source": [
    "This next snippet creates ds9 region file for the selected sources, one can edit as desired:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6ca7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_directory = './'  \n",
    "\n",
    "ra_column = ' ra'  \n",
    "dec_column = ' dec' \n",
    "isl_column = ' isl'\n",
    "\n",
    "for folder in os.listdir(base_directory):\n",
    "    folder_path = os.path.join(base_directory, folder)\n",
    "    \n",
    "    if os.path.isdir(folder_path):\n",
    "        filtered_csv_file = os.path.join(folder_path, 'RMcatalog_real_filtered.csv')\n",
    "        \n",
    "        if os.path.exists(filtered_csv_file):\n",
    "            df = pd.read_csv(filtered_csv_file)\n",
    "\n",
    "            # Create a DS9 region file\n",
    "            region_file = os.path.join(folder_path, 'r_regions.reg')\n",
    "            \n",
    "            with open(region_file, 'w') as f:\n",
    "                f.write(\"# Region file format: DS9 version 4.1\\n\")\n",
    "                f.write(\"global color=red dashlist=8 3 width=1 font=\\\"helvetica 10 normal roman\\\" select=1 highlite=1 dash=0 fixed=0 edit=1 move=1 delete=1 include=1 source=1\\n\")\n",
    "                f.write(\"fk5\\n\")\n",
    "                \n",
    "                for index, row in df.iterrows():\n",
    "                    ra = row[ra_column]\n",
    "                    dec = row[dec_column]\n",
    "                    isl_n=row[isl_column]\n",
    "                    f.write(f\"circle({ra}, {dec}, 200\\\") # text={{{isl_n}}}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809dedbe",
   "metadata": {},
   "source": [
    "### Step 2: Faraday Depth Spectra (FDF) Inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bef3f9",
   "metadata": {},
   "source": [
    "After the first selection one should check FDFs to make sure the selected source is real. With the next code you visualize each FDF and Q/U from the catalogs created for each pointing. The data are in the folder QUspec.\n",
    "It can be edited as desired of course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550edc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_directory = './'  \n",
    "%matplotlib inline\n",
    "def calculate_fdf(q, u):\n",
    "    return np.sqrt(q**2 + u**2)\n",
    "\n",
    "isl_column = ' isl'\n",
    "field_column = 'field'\n",
    "mad_column = 'dFDFcorMAD'\n",
    "th_column = 'dFDFth'\n",
    "snr_column = 'SNR_th'\n",
    "rm_column = ' RMspo'\n",
    "pol_column = ' polint_spo'\n",
    "\n",
    "for folder in os.listdir(base_directory):\n",
    "    folder_path = os.path.join(base_directory, folder)\n",
    "    \n",
    "    if os.path.isdir(folder_path):\n",
    "        filtered_csv_file = os.path.join(folder_path, 'RMcatalog_real_filtered.csv')\n",
    "        print(filtered_csv_file)\n",
    "        if os.path.exists(filtered_csv_file):\n",
    "            df = pd.read_csv(filtered_csv_file)\n",
    "\n",
    "            # \n",
    "            for index, row in df.iterrows():\n",
    "                isl_n=row[isl_column]\n",
    "                field=row[field_column]\n",
    "                \n",
    "                mad=row[mad_column]\n",
    "                th=row[th_column]\n",
    "                \n",
    "                snr=int(row[snr_column])\n",
    "                rm=row[rm_column]\n",
    "                polint=row[pol_column]\n",
    "         \n",
    "                filename_fdf = os.path.join(folder_path,'QUspec',f'lofar_QU_err_{isl_n}_FDFclean.dat')  \n",
    "                filename_cl = os.path.join(folder_path,'QUspec',f'lofar_QU_err_{isl_n}_FDFmodel.dat')\n",
    "\n",
    "                data = np.loadtxt(filename_fdf)\n",
    "                cleanc=np.loadtxt(filename_cl)\n",
    "                # Extract columns\n",
    "                x = data[:, 0]\n",
    "                q = data[:, 1]\n",
    "                u = data[:, 2]\n",
    "\n",
    "                # Extract columns\n",
    "                x_c = cleanc[:, 0]\n",
    "                q_c = cleanc[:, 1]\n",
    "                u_c = cleanc[:, 2]\n",
    "\n",
    "\n",
    "                # Calculate \n",
    "                fdf = calculate_fdf(q, u)\n",
    "                clc = calculate_fdf(q_c, u_c)\n",
    "               \n",
    "                fig, axs = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "                print('Field', field)\n",
    "                axs[0].plot(x, fdf, linestyle='-', color='dodgerblue', linewidth=0.8)\n",
    "                #axs[0].plot(x_c, clc, linestyle='-', color='red', label='Clean comp.')\n",
    "                axs[0].plot(rm, polint, marker='x', color='red')\n",
    "                axs[0].axvline(rm, 0, color='red', linestyle='--', linewidth=0.6, label='Real peak')\n",
    "                axs[0].axhline(th, 0, color='navy', linestyle='--', linewidth=1, label=r'$\\sigma_{QU}$', zorder=5)\n",
    "                #axs[0].axhline(mad, 0, color='orange', linestyle='--', linewidth=0.8,label='mad', zorder=5)\n",
    "                axs[0].set_xlabel(r'$\\phi$ $[rad/m^{2}]$ ')\n",
    "                axs[0].set_ylabel(r'|FDF| [Jy beam$^{-1}$]')\n",
    "                axs[0].set_title(f'{field} - Plot of FDF {isl_n} - SNR {snr}')\n",
    "                axs[0].set_xlim(-400, 400)\n",
    "                axs[0].legend(loc='upper right')\n",
    "                axs[0].grid(True, color='grey', linestyle='--', linewidth=0.2, alpha=0.5, zorder=0)\n",
    "                \n",
    "                #axs[1].plot(x, fdf, linestyle='-', color='b',alpha=0.7, label='PI')\n",
    "                axs[1].plot(x, q, linestyle='-', color='green', alpha=0.7, label='Re (Q)')\n",
    "                axs[1].plot(x, u, linestyle='-', color='gold', alpha=0.7, label='Im (U)')\n",
    "                #axs[1].plot(rm, polint, marker='x', color='green')\n",
    "                axs[1].axhline(th, 0, color='black', linestyle='--', linewidth=1, label=r'$\\sigma_{QU}$', zorder=5)\n",
    "                axs[1].axhline(-th, 0, color='black', linestyle='--', linewidth=1, zorder=5)\n",
    "                axs[1].set_ylim(-5*th, 5*th)\n",
    "                axs[1].set_xlim(-400, 400)\n",
    "                #axs[1].fill_between(x, mad, -mad, color='grey', label='dFDFcorMAD', alpha=0.7, zorder=15)\n",
    "                axs[1].set_xlabel(r'$\\phi$ $[rad/m^{2}]$ ')\n",
    "                axs[1].set_ylabel(r'[Jy beam$^{-1}$]')\n",
    "                #axs[1].set_title(f'{field} - Plot of FDF {isl_n} - SNR {snr}')\n",
    "                axs[1].legend(loc='upper right')\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                #plt.savefig(f'./field_{field}_isl{isl_n}_snr{snr}.pdf')\n",
    "                plt.show()\n",
    "                # Close the current figure\n",
    "                plt.close()\n",
    "\n",
    "\n",
    "plt.ioff()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f105b915",
   "metadata": {},
   "source": [
    "At this point, you may want to go back to remove additional sources from each catalog. When you have a final selection for each pointing, you can merge them to produce a final catalog for each supercluster (completely optional, just what I did to keep things in order):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0c8e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "folder_pattern = './catalogs/superclusters/278/nonDR2/rmsynth_*'  # Adjust this pattern based on your folder structure\n",
    "\n",
    "folders = glob.glob(folder_pattern)\n",
    "#print(folders)\n",
    "\n",
    "combined_df = pd.DataFrame()\n",
    "\n",
    "for folder in folders:\n",
    "    file_pattern = os.path.join(folder, 'RMcatalog_real_filtered.csv')\n",
    "    file_paths = glob.glob(file_pattern)\n",
    "    \n",
    "    for file_path in file_paths:\n",
    "        df = pd.read_csv(file_path)\n",
    "        combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "\n",
    "# Save the combined df to a new CSV file\n",
    "combined_df.to_csv('./final_real_sources.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a32635",
   "metadata": {},
   "source": [
    "LOFAR pointings overlap. To remove duplicate sources, one keeps the one with higher SNR:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fbefd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from astropy.coordinates import SkyCoord\n",
    "import astropy.units as u\n",
    "import glob\n",
    "import os\n",
    "\n",
    "\n",
    "def centers_duplicates(coord1, coord2, shift=100 * u.arcsecond):\n",
    "    return coord1.separation(coord2) <= shift\n",
    "\n",
    "base_directory = './catalogs/superclusters/'  \n",
    "\n",
    "\n",
    "for folder in os.listdir(base_directory):\n",
    "    folder_path = os.path.join(base_directory, folder)\n",
    "    \n",
    "    if os.path.isdir(folder_path):\n",
    "        filtered_csv_file = os.path.join(folder_path, 'final_real_sources.csv')\n",
    "        \n",
    "        if os.path.exists(filtered_csv_file):\n",
    "            df = pd.read_csv(filtered_csv_file)\n",
    "\n",
    "            rows_to_drop = []\n",
    "\n",
    "            for i, row1 in df.iterrows():\n",
    "                coord1 = SkyCoord(row1[' ra'], row1[' dec'], unit='deg')\n",
    "\n",
    "                if i not in rows_to_drop:\n",
    "                    for j, row2 in df.loc[df.index > i].iterrows():\n",
    "                        coord2 = SkyCoord(row2[' ra'], row2[' dec'], unit='deg')\n",
    "                        \n",
    "                        if centers_duplicates(coord1, coord2):\n",
    "                            rows_to_drop.append(i) if row1['SNR_th'] <= row2['SNR_th'] else rows_to_drop.append(j)\n",
    "\n",
    "            df_cleaned = df.drop(index=rows_to_drop)\n",
    "\n",
    "            # Reset index\n",
    "            df_cleaned.reset_index(drop=True, inplace=True)\n",
    "            df_cleaned.to_csv(filtered_csv_file, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fdb51a",
   "metadata": {},
   "source": [
    "### Galactic RM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5f5051",
   "metadata": {},
   "source": [
    "At this point, it would be easier to work with fits files. You can easily convert csv to fits with topcat. \n",
    "\n",
    "To compute the GRM at the location of the final sources:\n",
    "\n",
    "First, you'll need to download the relevant galactic map file from here: https://wwwmpa.mpa-garching.mpg.de/~ensslin/research/data/faraday_revisited.html. Then make sure that you are reading the correct columns name (these were mine). [If you use this for NVSS sources, change the parameter nside to 256]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3695d48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.io import fits\n",
    "import healpy as hp\n",
    "import numpy as np\n",
    "#from astropy_healpix import HEALPix\n",
    "#from astropy_healpix import healpy as hp\n",
    "import matplotlib.pyplot as plt\n",
    "import astropy.units as u\n",
    "from astropy.coordinates import SkyCoord\n",
    "import h5py\n",
    "import math as m\n",
    "from astropy.stats import mad_std\n",
    "\n",
    "# https://wwwmpa.mpa-garching.mpg.de/~ensslin/research/data/faraday_revisited.html\n",
    "#file = 'faraday_sky_w_ff.hdf5' #noLOFAR\n",
    "file = 'faraday2020v2.hdf5' #LOFAR\n",
    "f = h5py.File(file, 'r')\n",
    "\n",
    "#fsky = f[u'faraday sky']\n",
    "#n1 = fsky.get(u'mean')\n",
    "#n2 = fsky.get(u'std')\n",
    "\n",
    "n1 = f.get(u'faraday_sky_mean') #for LOFAR\n",
    "n2 = f.get(u'faraday_sky_std') #for LOFAR\n",
    "\n",
    "\n",
    "RMmap = np.array(n1)\n",
    "RMerrmap = np.array(n2)\n",
    "\n",
    "#catalogpath= '/Users/shane/Documents/LOFAR/GRG_LOTSS/DDFacet_cube/DR2_polcats/polcat_May2020/'\n",
    "table = 'catalog.fits'\n",
    "tabledata = fits.open(table)[1].data\n",
    "\n",
    "ra = tabledata.field('ra')\n",
    "dec = tabledata.field('dec')\n",
    "rm_peak = tabledata.field('phiPeakPIfit_rm2')\n",
    "rm_peak_err = tabledata.field('dPhiPeakPIfit_rm2')\n",
    "\n",
    "#rm_peak = tabledata.field('rm')\n",
    "#rm_peak_err = tabledata.field('rm_err')\n",
    "\n",
    "\n",
    "c_icrs = SkyCoord(ra=ra, dec=dec, frame='icrs', unit='deg')\n",
    "\n",
    "glon = c_icrs.galactic.l.degree\n",
    "glat = c_icrs.galactic.b.degree \n",
    "\n",
    "#####################\n",
    "# average over disk #\n",
    "#####################\n",
    "GRM = np.zeros(len(ra))\n",
    "GRMerr = np.zeros(len(ra))\n",
    "RRM = np.zeros(len(ra))\n",
    "RRMerr = np.zeros(len(ra))\n",
    "\n",
    "for i in range(len(ra)):\n",
    "    theta = glon[i]\n",
    "    phi = glat[i]\n",
    "      #GRMval = hp.pixelfunc.get_interp_val(RMmap, theta, phi, lonlat=True)\n",
    "      #print glon[i], glat[i]\n",
    "    vec = hp.ang2vec(glon[i], glat[i], lonlat=True)\n",
    "    ipix_disc = hp.query_disc(nside=512, vec=vec, radius=np.radians(0.5))  #nside=512 lofar, nside=256 not lofar\n",
    "    GRMval = np.mean(RMmap[ipix_disc])\n",
    "    GRM[i] = GRMval\n",
    "      #GRMerrval = hp.pixelfunc.get_interp_val(RMerrmap, theta, phi, lonlat=True)\n",
    "      #GRMerrval = np.mean(RMerrmap[ipix_disc]) \n",
    "    GRMerrval = np.mean(RMerrmap[ipix_disc])/m.sqrt(len(ipix_disc)) #consistent with Carretti\n",
    "    GRMerr[i] = GRMerrval\n",
    "    \n",
    "    RRM[i]=rm_peak[i]-GRMval\n",
    "    RRMerr[i]=rm_peak_err[i]+GRMerrval\n",
    "\n",
    "# add columns to fits table\n",
    "orig_table = fits.open(table)[1].data\n",
    "orig_cols = orig_table.columns\n",
    "new_cols = fits.ColDefs([\n",
    "  #fits.Column(name='l', format='D', array=glon),\n",
    "  #fits.Column(name='b', format='D', array=glat),\n",
    "  fits.Column(name='GRM_wff_v2022', format='D', array=GRM),\n",
    "  fits.Column(name='GRMerr_wff_v2022', format='D', array=GRMerr),\n",
    "  fits.Column(name='RRM_v2022', format='D', array=RRM),\n",
    "  fits.Column(name='RRMerr_v2022', format='D', array=RRMerr)])\n",
    "hdu = fits.BinTableHDU.from_columns(orig_cols + new_cols)\n",
    "hdu.writeto('./name.fits',overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42b20a4",
   "metadata": {},
   "source": [
    "### LoTSS DR2 and NVSS selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242aefec",
   "metadata": {},
   "source": [
    "Now you want sources from other catalogs that cover the extension of the supercluster. I used DR2 and NVSS. From the catalogs, I defined a radius around the center of the supercluster and selected al sources withing that radius. At the end, it creates a catalog and a ds9 region file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23b0ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from astropy.io import fits\n",
    "import astropy.units as u\n",
    "from astropy.coordinates import SkyCoord\n",
    "from astropy.table import Table\n",
    "\n",
    "\n",
    "def process_catalog(ra_c, dec_c, mscc):\n",
    "    #catalog_path = os.path.join('catalogs/superclusters', 'NVSS.fits')\n",
    "    catalog_path = os.path.join('catalogs/superclusters', 'LoTSS_DR2_RMGrid_v1_RMTable.fits')\n",
    "    if os.path.exists(catalog_path):\n",
    "        catalog = Table.read(catalog_path, format='fits')\n",
    "        selected_points = []\n",
    "        \n",
    "        target_coord = SkyCoord(ra=ra_c*u.deg, dec=dec_c*u.deg, frame='icrs')\n",
    "\n",
    "        for row in catalog:\n",
    "            ra_j2000_str = row['ra']\n",
    "            dec_j2000_str = row['dec']\n",
    "\n",
    "            # Convert RA and Dec from string to SkyCoord --- this is needed if coords not in deg already\n",
    "            #catalog_coord = SkyCoord(ra=ra_j2000_str, dec=dec_j2000_str, unit=(u.hourangle, u.deg))\n",
    "            catalog_coord = SkyCoord(ra=ra_j2000_str*u.deg, dec=dec_j2000_str*u.deg, frame='icrs')\n",
    "\n",
    "            if target_coord.separation(catalog_coord).degree <= 19:\n",
    "                selected_points.append((catalog_coord.ra.deg, catalog_coord.dec.deg))\n",
    "                \n",
    "                \n",
    "        create_ds9_region_file(selected_points, ra_c, dec_c, mscc)\n",
    "        \n",
    "        selected_catalog = catalog.copy()\n",
    "        mask = [False] * len(selected_catalog)\n",
    "        for i, row in enumerate(selected_catalog):\n",
    "            #catalog_coord = SkyCoord(ra=row['ra'], dec=row['dec'], unit=(u.hourangle, u.deg))\n",
    "            catalog_coord = SkyCoord(ra=row['ra']*u.deg, dec=row['dec']*u.deg, frame='icrs')\n",
    "\n",
    "            if target_coord.separation(catalog_coord).degree <= 19:\n",
    "                mask[i] = True\n",
    "\n",
    "        selected_catalog = selected_catalog[mask]\n",
    "        #selected_catalog.write(os.path.join('catalogs/superclusters', f'{mscc}_nvss_rm_cat.fits'), format='fits', overwrite=True)\n",
    "        selected_catalog.write(os.path.join('catalogs/superclusters', f'{mscc}_lofar_rm_cat.fits'), format='fits', overwrite=True)\n",
    "\n",
    "\n",
    "        \n",
    "def create_ds9_region_file(points, ra_c, dec_c, mscc):\n",
    "    #region_file_path = os.path.join('catalogs/superclusters', f'{mscc}_nvss_rm_cat_points.reg')\n",
    "    region_file_path = os.path.join('catalogs/superclusters', f'{mscc}_lofar_rm_cat_points.reg')\n",
    "    \n",
    "    with open(region_file_path, 'w') as f:\n",
    "        f.write(\"# Region file format: DS9 version 4.1\\n\")\n",
    "        f.write(\"global color=red dashlist=8 3 width=1 font=\\\"helvetica 10 normal roman\\\" select=1 highlite=1 dash=0 fixed=0 edit=1 move=1 delete=1 include=1 source=1\\n\")\n",
    "        f.write(\"fk5\\n\")\n",
    "        for ra, dec in points:\n",
    "            f.write(f'point({ra}, {dec}) # point=cross\\n')\n",
    "    print(f\"DS9 region file created for {ra_c}, {dec_c}: {region_file_path}\")\n",
    "\n",
    "\n",
    "#This is the starting file containing a list of superclusters with their center coordinates. \n",
    "fits_file = Table.read('catalogs/superclusters/mscc_zdec_select_final.fits')\n",
    "\n",
    "for row in fits_file:\n",
    "    ra_c = row['RAJ2000']  # Assuming 'ra' is the column name\n",
    "    dec_c = row['DEJ2000']\n",
    "    mscc = row ['MSCC'] # Assuming 'dec' is the column name\n",
    "    process_catalog(ra_c,dec_c, mscc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d860f9ee",
   "metadata": {},
   "source": [
    "#### Note: the removal of duplicates between catalogs can be easily done in Topcat. Which is what I did: you select the two catalogs and the radius inside of which it finds a duplicate (NVSS resolution). Otherwise you can use similar code as before. Between NVSS and LOFAR duplicates I kept LOFAR."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0a46e8",
   "metadata": {},
   "source": [
    "### Density cubes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd87b8c9",
   "metadata": {},
   "source": [
    "The catalog of superclusters I used is this one: https://cdsarc.u-strasbg.fr/viz-bin/cat/J/A+A/637/A31,\n",
    "and the density profile is the universal galaxy cluster density profile from Pratt+2022 (https://www.aanda.org/articles/aa/abs/2022/09/aa43074-22/aa43074-22.html).\n",
    "\n",
    "To create the density cubes you'll need the catalog with the coordinates, redshift, virial radius of each supercluster member. I run this on lofar6 but should work on normal machine also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f47c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from astropy.cosmology import FlatLambdaCDM\n",
    "import astropy.units as u\n",
    "from astropy.io import fits\n",
    "from astropy.wcs import WCS\n",
    "\n",
    "cosmo = FlatLambdaCDM(H0=70, Om0=0.3)\n",
    "\n",
    "\n",
    "def compute_r500(Rv, zred):\n",
    "    cosmo = FlatLambdaCDM(H0=70, Om0=0.3)\n",
    "    kpc_per_degree = cosmo.kpc_proper_per_arcmin(zred).to(u.kpc / u.degree).value\n",
    "    R500 = Rv * (0.1**(1/3))  # deg\n",
    "    R500_cm = R500 * kpc_per_degree * (3.086e21)  # cm\n",
    "    return R500, R500_cm\n",
    "\n",
    "def compute_m500_and_norm(R500_cm, zred):\n",
    "    msun = 1.989e+33  # g\n",
    "    omegaM = 0.3\n",
    "    omegaL = 0.7\n",
    "    h70 = 0.7\n",
    "    alpha_r = 2.09\n",
    "    alpha_m = 0.22\n",
    "    \n",
    "    rho_500 = 4.6e-27 * ((omegaM * (1 + zred)**3) + omegaL)\n",
    "    M500 = ((4 * np.pi / 3) * rho_500 * (R500_cm)**3) / msun\n",
    "    norm = ((((omegaM * (1 + zred)**3) + omegaL)**(1/2))**alpha_r) * (((M500 * h70) / (5 * 10**14))**alpha_m)\n",
    "    \n",
    "    return rho_500, M500, norm\n",
    "\n",
    "def gnfw_density(xx, norm, rho_500):\n",
    "    f0 = 1.20\n",
    "    xs = 0.28\n",
    "    a = 0.42\n",
    "    b = 0.78\n",
    "    g = 1.52\n",
    "    \n",
    "    rho_m = (f0 * norm) / (((xx / xs)**a) * (1 + (xx / xs)**g)**((3 * b - a) / g))    \n",
    "    return rho_m * rho_500\n",
    "\n",
    "def median_density_3d(x, y, z, x0, y0, z0, R500, R500_cm, norm, rho_500):\n",
    "    r = np.sqrt((x - x0)**2 + (y - y0)**2 + (z - z0)**2)\n",
    "    xx = r / R500\n",
    "    density = np.zeros_like(xx)\n",
    "    valid_mask = xx <= 10\n",
    "    density[valid_mask] = gnfw_density(xx[valid_mask], norm, rho_500)\n",
    "    \n",
    "    return density\n",
    "\n",
    "def calculations(clusters, cosmo):\n",
    "    calcs = []\n",
    "    for cluster in clusters:\n",
    "        ra0, dec0, z0, Rv, zred = cluster\n",
    "        R500, R500_cm = compute_r500(Rv, zred)\n",
    "        rho_500, M500, norm = compute_m500_and_norm(R500_cm, zred)\n",
    "        calcs.append((ra0, dec0, z0, R500, R500_cm, norm, rho_500))\n",
    "    return calcs\n",
    "\n",
    "def create_density_cube(ra_range, dec_range, z_range, clusters, pixel_scale, z_resolution):\n",
    "    ra_min, ra_max = ra_range\n",
    "    dec_min, dec_max = dec_range\n",
    "    z_min, z_max = z_range\n",
    "    \n",
    "    ra_size = int((ra_max - ra_min) / pixel_scale)\n",
    "    dec_size = int((dec_max - dec_min) / pixel_scale)\n",
    "    z_size = z_resolution\n",
    "    print(ra_size, dec_size)\n",
    "    \n",
    "    cosmo = FlatLambdaCDM(H0=70, Om0=0.3)\n",
    "    \n",
    "    ra_grid = np.linspace(ra_max, ra_min, ra_size)\n",
    "    dec_grid = np.linspace(dec_min, dec_max, dec_size)\n",
    "    z_grid = np.linspace(z_min, z_max, z_size)\n",
    "    \n",
    "    lum_distances = cosmo.luminosity_distance(z_grid).to(u.Mpc).value\n",
    "    lum_distances_deg = (lum_distances * 1e3) / cosmo.kpc_proper_per_arcmin(z_grid).to(u.kpc / u.degree).value\n",
    "    \n",
    "    density_cube = np.zeros((ra_size, dec_size, z_size))\n",
    "    \n",
    "    precomputed_clusters = calculations(clusters, cosmo)\n",
    "    \n",
    "    # Create meshgrid for all grid points\n",
    "    ra_grid_mesh, dec_grid_mesh, z_grid_mesh = np.meshgrid(ra_grid, dec_grid, lum_distances_deg, indexing='ij')\n",
    "    \n",
    "    for vals in precomputed_clusters:\n",
    "        ra0, dec0, z0, R500, R500_cm, norm, rho_500 = vals\n",
    "        ra_pix = int((ra0 - ra_min) / pixel_scale)\n",
    "        dec_pix = int((dec0 - dec_min) / pixel_scale)\n",
    "        z_pix = int((z0 - z_min) * z_size / (z_max - z_min))\n",
    "        \n",
    "        density_values = median_density_3d(\n",
    "            ra_grid_mesh, dec_grid_mesh, z_grid_mesh,\n",
    "            ra0, dec0, lum_distances_deg[z_pix], R500, R500_cm, norm, rho_500\n",
    "        )\n",
    "        \n",
    "        density_cube += density_values\n",
    "    \n",
    "    return density_cube\n",
    "\n",
    "def adjust_wcs(fits_file, ra_size, dec_size, z_size, pixel_scale, z_resolution, ra_range, dec_range, z_range):\n",
    "    ra_min, ra_max = ra_range\n",
    "    dec_min, dec_max = dec_range\n",
    "    z_min, z_max = z_range\n",
    "\n",
    "    with fits.open(fits_file) as hdul:\n",
    "        w = WCS(hdul[0].header)\n",
    "        w = WCS(naxis=3)\n",
    "    \n",
    "    w.wcs.crpix = [ra_size / 2, dec_size / 2, z_size / 2]\n",
    "    w.wcs.cdelt = [pixel_scale, pixel_scale, (z_max - z_min) / z_size]\n",
    "    w.wcs.crval = [ra_min + (ra_max - ra_min) / 2, dec_min + (dec_max - dec_min) / 2, (z_min + z_max) / 2]\n",
    "    w.wcs.ctype = [\"RA---TAN\", \"DEC--TAN\", \"VELO-LSR\"]\n",
    "    \n",
    "    return w\n",
    "\n",
    "def save_to_fits(data, filename):\n",
    "    hdu = fits.PrimaryHDU(data)\n",
    "    #hdu.header.update(wcs.to_header())\n",
    "    hdul = fits.HDUList([hdu])\n",
    "    hdul.writeto(filename, overwrite=True)\n",
    "\n",
    "def read_clusters_from_fits(filename):\n",
    "    with fits.open(filename) as hdul:\n",
    "        data = hdul[1].data\n",
    "        clusters = [(row['RAJ2000'], row['DEJ2000'], row['z'], row['Rvir'], row['z']) for row in data]\n",
    "    return clusters\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######################## These were the superclusters I analyzed  #############################\n",
    "\n",
    "#474 \n",
    "ra_range = [221, 257]  # RA range in degrees\n",
    "dec_range = [0, 34]  # Dec range in degrees\n",
    "z_range = [0.025, 0.05]  # Redshift range\n",
    "pixel_scale = 100 / 3600  # Pixel scale in degrees\n",
    "z_resolution = 100  # Number of grid points along the redshift dimension\n",
    "\n",
    "# Read clusters from FITS file\n",
    "cluster_fits_file = 'catalogs/superclusters/474/mscc_474_cat.fits'\n",
    "clusters = read_clusters_from_fits(cluster_fits_file)\n",
    "\n",
    "# Create density cube\n",
    "density_cube = create_density_cube(ra_range, dec_range, z_range, clusters, pixel_scale, z_resolution)\n",
    "save_to_fits(density_cube, 'catalogs/superclusters/474/density_cube_mesh.fits')\n",
    "\n",
    "\n",
    "\n",
    "#278\n",
    "ra_range = [148, 189]  \n",
    "dec_range = [6, 46]  \n",
    "z_range = [0.02, 0.045]  \n",
    "pixel_scale = 100 / 3600 \n",
    "z_resolution = 100 \n",
    "\n",
    "# Read clusters from FITS file\n",
    "cluster_fits_file = 'catalogs/superclusters/278/mscc_278_cat.fits'\n",
    "clusters = read_clusters_from_fits(cluster_fits_file)\n",
    "\n",
    "# Create density cube\n",
    "density_cube = create_density_cube(ra_range, dec_range, z_range, clusters, pixel_scale, z_resolution)\n",
    "save_to_fits(density_cube, 'catalogs/superclusters/278/density_cube_mesh.fits')\n",
    "\n",
    "\n",
    "#463\n",
    "ra_range = [215, 248] \n",
    "dec_range = [12, 44]  \n",
    "z_range = [0.05, 0.095] \n",
    "pixel_scale = 100 / 3600 \n",
    "z_resolution = 100  \n",
    "\n",
    "# Read clusters from FITS file\n",
    "cluster_fits_file = 'catalogs/superclusters/463/mscc_463_cat.fits'\n",
    "clusters = read_clusters_from_fits(cluster_fits_file)\n",
    "\n",
    "# Create density cube\n",
    "density_cube = create_density_cube(ra_range, dec_range, z_range, clusters, pixel_scale, z_resolution)\n",
    "save_to_fits(density_cube, 'catalogs/superclusters/463/density_cube_mesh.fits')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbeeb34e",
   "metadata": {},
   "source": [
    "This is to attach a WCS to the cube. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a15803",
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.wcs import WCS\n",
    "from astropy.io import fits\n",
    "\n",
    "def create_wcs(ra_range, dec_range, z_range, pixel_scale, z_resolution):\n",
    "    ra_min, ra_max = ra_range\n",
    "    dec_min, dec_max = dec_range\n",
    "    z_min, z_max = z_range\n",
    "    \n",
    "    # Define the WCS object\n",
    "    wcs = WCS(naxis=3)\n",
    "    \n",
    "    # Axis 1: RA\n",
    "    wcs.wcs.crpix[0] = (ra_max - ra_min) / (2 * pixel_scale)\n",
    "    wcs.wcs.cdelt[0] = pixel_scale  # pixel size in degrees\n",
    "    wcs.wcs.crval[0] = (ra_max + ra_min) / 2\n",
    "    wcs.wcs.ctype[0] = 'RA---SIN'\n",
    "    wcs.wcs.cunit[0] = 'deg'\n",
    "    \n",
    "    # Axis 2: Dec\n",
    "    wcs.wcs.crpix[1] = (dec_max - dec_min) / (2 * pixel_scale)\n",
    "    wcs.wcs.cdelt[1] = pixel_scale # pixel size in degrees\n",
    "    wcs.wcs.crval[1] = (dec_max + dec_min) / 2\n",
    "    wcs.wcs.ctype[1] = 'DEC--SIN'\n",
    "    wcs.wcs.cunit[1] = 'deg'\n",
    "    \n",
    "    # Axis 3: Redshift\n",
    "    wcs.wcs.crpix[2] = z_resolution / 2\n",
    "    wcs.wcs.cdelt[2] = (z_max - z_min) / z_resolution\n",
    "    wcs.wcs.crval[2] = (z_max + z_min) / 2\n",
    "    wcs.wcs.ctype[2] = 'REDSHIFT'\n",
    "    wcs.wcs.cunit[2] = ''\n",
    "    \n",
    "    # Additional keywords\n",
    "    wcs.wcs.radesys = 'ICRS'\n",
    "    wcs.wcs.lonpole = 180.0\n",
    "    wcs.wcs.latpole = 17.449952777778\n",
    "    \n",
    "    return wcs\n",
    "\n",
    "def save_density_cube_with_wcs(input_fits, output_fits, ra_range, dec_range, z_range, pixel_scale, z_resolution):\n",
    "    # Read the existing density cube from FITS file\n",
    "    ra_min, ra_max = ra_range\n",
    "    dec_min, dec_max = dec_range\n",
    "    z_min, z_max = z_range\n",
    "\n",
    "    with fits.open(input_fits) as hdul:\n",
    "        density_cube = hdul[0].data\n",
    "    \n",
    "    # Create WCS object\n",
    "    wcs = create_wcs(ra_range, dec_range, z_range, pixel_scale, z_resolution)\n",
    "    \n",
    "    # Create the FITS header\n",
    "    header = wcs.to_header()\n",
    "\n",
    "    header['CRPIX1'] = wcs.wcs.crpix[0]\n",
    "    header['CRPIX2'] = wcs.wcs.crpix[1]\n",
    "    header['CRPIX3'] = wcs.wcs.crpix[2]\n",
    "    \n",
    "    header['CDELT1'] = -wcs.wcs.cdelt[0]\n",
    "    header['CDELT2'] = wcs.wcs.cdelt[1]\n",
    "    header['CDELT3'] = wcs.wcs.cdelt[2]\n",
    "    \n",
    "    header['CRVAL1'] = wcs.wcs.crval[0]\n",
    "    header['CRVAL2'] = wcs.wcs.crval[1]\n",
    "    header['CRVAL3'] = wcs.wcs.crval[2]\n",
    "    \n",
    "    header['CTYPE1'] = wcs.wcs.ctype[0]\n",
    "    header['CTYPE2'] = wcs.wcs.ctype[1]\n",
    "    header['CTYPE3'] = wcs.wcs.ctype[2]\n",
    "    \n",
    "    header['CUNIT1'] = str(wcs.wcs.cunit[0])\n",
    "    header['CUNIT2'] = str(wcs.wcs.cunit[1])\n",
    "    header['CUNIT3'] = str(wcs.wcs.cunit[2])\n",
    "    \n",
    "    header['RADESYS'] = wcs.wcs.radesys\n",
    "    header['LONPOLE'] = wcs.wcs.lonpole\n",
    "    header['LATPOLE'] = wcs.wcs.latpole\n",
    "    \n",
    "    # Create the Primary HDU with the density cube data and WCS header\n",
    "    hdu = fits.PrimaryHDU(density_cube, header=header)\n",
    "    \n",
    "    # Write to a new FITS file\n",
    "    hdul_new = fits.HDUList([hdu])\n",
    "    hdul_new.writeto(output_fits, overwrite=True)\n",
    "\n",
    "#474\n",
    "#ra_range = [221, 257]  # RA range in degrees\n",
    "#dec_range = [0, 34]  # Dec range in degrees\n",
    "#z_range = [0.025, 0.05]  # Redshift range\n",
    "#pixel_scale = 100 / 3600  # Pixel scale in degrees\n",
    "#z_resolution = 100  # Number of grid points along the redshift dimension\n",
    "#save_density_cube_with_wcs('catalogs/superclusters/474/density_cube_mesh_474.fits', 'catalogs/superclusters/474/density_cube_mesh_474_wcs.fits', ra_range, dec_range, z_range, pixel_scale, z_resolution)\n",
    "\n",
    "#278\n",
    "\n",
    "ra_range = [148, 189]  # RA range in degrees\n",
    "dec_range = [6, 46]  # Dec range in degrees\n",
    "z_range = [0.02, 0.045]  # Redshift range\n",
    "pixel_scale = 100 / 3600  # Pixel scale in degrees\n",
    "z_resolution = 100  # Number of grid points along the redshift dimension\n",
    "save_density_cube_with_wcs('catalogs/superclusters/278/density_cube_mesh_278.fits', 'catalogs/superclusters/278/3D/density_cube_mesh_278_wcs.fits', ra_range, dec_range, z_range, pixel_scale, z_resolution)\n",
    "\n",
    "#463\n",
    "#ra_range = [215, 248]  # RA range in degrees\n",
    "#dec_range = [12, 44]  # Dec range in degrees\n",
    "#z_range = [0.05, 0.095]  # Redshift range\n",
    "#pixel_scale = 100 / 3600  # Pixel scale in degrees\n",
    "#z_resolution = 100  # Number of grid points along the redshift dimension\n",
    "#save_density_cube_with_wcs('catalogs/superclusters/463/density_cube_mesh_463.fits', 'catalogs/superclusters/463/3D/density_cube_mesh_463_wcs.fits', ra_range, dec_range, z_range, pixel_scale, z_resolution)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0151dc5d",
   "metadata": {},
   "source": [
    "At this point we can compute the mean density crossed by the polarized emission of each catalogued source. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8f21f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.io import fits\n",
    "from astropy.wcs import WCS\n",
    "import numpy as np\n",
    "\n",
    "def mean_density_per_source(density_cube_fits, catalog_fits, output_catalog_fits):\n",
    "    with fits.open(density_cube_fits) as hdul:\n",
    "        density_cube = hdul[0].data\n",
    "        print(density_cube.shape[0],density_cube.shape[1], density_cube.shape[2])\n",
    "        wcs = WCS(hdul[0].header)\n",
    "        print(wcs)\n",
    "        \n",
    "    # Read the catalog of sources\n",
    "    with fits.open(catalog_fits) as hdul:\n",
    "        catalog_data = hdul[1].data\n",
    "        catalog_header = hdul[1].header\n",
    "    \n",
    "    mean_densities = np.zeros(len(catalog_data))\n",
    "\n",
    "    for i, source in enumerate(catalog_data):\n",
    "        ra = source['ra']\n",
    "        dec = source['dec']\n",
    "        #print(dec)\n",
    "        \n",
    "        # Convert RA and Dec to pixel coordinates\n",
    "        #print(ra,dec)\n",
    "        pixel_coords = wcs.world_to_pixel_values(ra, dec, 1)\n",
    "        x_pix, y_pix = pixel_coords[0], pixel_coords[1]\n",
    "        #print(x_pix, y_pix)\n",
    "      \n",
    "    # Check if the coordinates are within bounds and are not NaN\n",
    "        if not np.isnan(x_pix) and not np.isnan(y_pix):\n",
    "            x_pix, y_pix = int(x_pix), int(y_pix)\n",
    "            if 0 <= x_pix < density_cube.shape[0] and 0 <= y_pix < density_cube.shape[1]:\n",
    "                # Extract the density values along the redshift axis\n",
    "                density_values = density_cube[x_pix, y_pix, :]\n",
    "                \n",
    "                # Compute the mean density over the redshift axis\n",
    "                mean_density = np.mean(density_values)\n",
    "                \n",
    "\n",
    "                mean_densities[i] = mean_density\n",
    "                \n",
    "                \n",
    "    # Add the mean density values as a new column in the catalog\n",
    "    col_mean_density = fits.Column(name='mean_density', format='E', array=mean_densities)\n",
    "    new_columns = fits.ColDefs([col_mean_density])\n",
    "    new_catalog_hdu = fits.BinTableHDU.from_columns(catalog_data.columns + new_columns, header=catalog_header)\n",
    "    \n",
    "    # Save the updated catalog to a new FITS file\n",
    "    new_catalog_hdu.writeto(output_catalog_fits, overwrite=True)\n",
    "\n",
    "# Example\n",
    "density_cube_fits = 'catalogs/superclusters/278/3D/density_cube_mesh_278_wcs.fits'\n",
    "catalog_fits = 'catalogs/superclusters/278/3D/278_nvss_rm_cat_grm.fits'\n",
    "output_catalog_fits = 'catalogs/superclusters/278/3D/278_nvss_rm_cat_grm_d.fits'\n",
    "\n",
    "mean_density_per_source(density_cube_fits, catalog_fits, output_catalog_fits)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6818a360",
   "metadata": {},
   "source": [
    "### RRM variance computation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a68007",
   "metadata": {},
   "source": [
    "In this part I read all the catalogs of sources, define the binning, and the weighting based on Rudnick 2019. I also tried to use bootstrapping for the error but ended up not using it (so ignore it). Sources are weighted based on their degree of polarization. \n",
    "##### Note: I had two superclusters overlapping each other on the los, which is why I have additional files 'overlap'. If this happened in your case too, I just isolated the overlapping region sources (count them once), and for those the density is the sum of the density through each supercluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13743006",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from astropy.table import Table\n",
    "from astropy.io import fits\n",
    "import fnmatch\n",
    "import matplotlib.pyplot as plt\n",
    "from astropy.stats import mad_std\n",
    "from astropy.cosmology import FlatLambdaCDM\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import bootstrap\n",
    "%matplotlib inline\n",
    "\n",
    "base_directory = './catalogs/superclusters/overlap_corr/3D_overlapcorr/no_zeros/'  \n",
    "\n",
    "cosmo = FlatLambdaCDM(H0=70, Om0=0.3)\n",
    "rho_c=cosmo.critical_density(0.07)\n",
    "#rho_c=8.5e-30\n",
    "print(rho_c)\n",
    "\n",
    "################ NVSS ##################\n",
    "\n",
    "#fig, axs = plt.subplots(1, 2, figsize=(9, 14), sharex=False)\n",
    "\n",
    "nvss_input_file_path1 = os.path.join(base_directory, 'nvss_cat_rm_allscc_nodup_final.fits')\n",
    "nvss_input_file_path2 = os.path.join(base_directory, 'overlap_nvss_final.fits')\n",
    "\n",
    "        \n",
    "catalog1_file = nvss_input_file_path1\n",
    "catalog2_file = nvss_input_file_path2\n",
    "\n",
    "            \n",
    "            # Read the two catalogs\n",
    "catalog1 = Table.read(catalog1_file) #nvss\n",
    "catalog2 = Table.read(catalog2_file) #overlap\n",
    "\n",
    "#RM and RMerr\n",
    "nvss_rm=catalog1['rm']\n",
    "nvss_rm_err=catalog1['rm_err_corr']\n",
    "nvss_rm2=catalog2['rm_1']\n",
    "nvss_rm_err2=catalog2['rm_err_corr']\n",
    "\n",
    "#GRM and RRM\n",
    "\n",
    "nvss_grm=catalog1['GRM_wff_v2022']\n",
    "nvss_grm_err=catalog1['GRMerr_wff_v2022']\n",
    "\n",
    "nvss_grm2=catalog2['GRM_wff_v2022']\n",
    "nvss_grm_err2=catalog2['GRMerr_wff_v2022']\n",
    "\n",
    "nvss_rrm=catalog1['RRM_v2022']\n",
    "nvss_rrm2=catalog2['RRM_v2022']\n",
    "\n",
    "#PolDEG and DENSITY\n",
    "\n",
    "nvss_pold=catalog1['fracpol']*100\n",
    "nvss_pold2=catalog2['fracpol_1']*100\n",
    "\n",
    "nvss_pold_e=catalog1['fracpol_err']*100\n",
    "nvss_pold_e2=catalog2['fracpol_err_1']*100\n",
    "\n",
    "nvss_dens=catalog1['mean_density']\n",
    "nvss_dens2=catalog2['density_TOT']\n",
    "\n",
    "#concatenate\n",
    "rm_nvss_combined = np.concatenate([nvss_rm, nvss_rm2])\n",
    "rrm_nvss_combined = np.concatenate([nvss_rrm, nvss_rrm2])\n",
    "\n",
    "rm_err_nvss_combined = np.concatenate([nvss_rm_err, nvss_rm_err2])\n",
    "grm_err_nvss_combined = np.concatenate([nvss_grm_err, nvss_grm_err2])\n",
    "\n",
    "poldeg_nvss_combined = np.concatenate([nvss_pold, nvss_pold2])\n",
    "rho_nvss_combined = np.concatenate ([nvss_dens, nvss_dens2])\n",
    "\n",
    "\n",
    "\n",
    "###################### LOFAR #######################\n",
    "\n",
    "\n",
    "dr2_input_file_path1 = os.path.join(base_directory, 'lofar_dr2_rm_allscc_nodup.fits')\n",
    "ndr2_input_file_path2 = os.path.join(base_directory, 'lofar_nondr2_rm_allscc_nodup.fits')\n",
    "dr2_input_file_path3 = os.path.join(base_directory, 'DR2_overlap.fits')\n",
    "ndr2_input_file_path4 = os.path.join(base_directory, 'nonDR2_overlap.fits')\n",
    "\n",
    "\n",
    "        \n",
    "dr2_catalog1_file = dr2_input_file_path1\n",
    "ndr2_catalog2_file = ndr2_input_file_path2\n",
    "dr2_catalog3_file = dr2_input_file_path3\n",
    "ndr2_catalog4_file = ndr2_input_file_path4\n",
    "\n",
    "            \n",
    "            # Read the two catalogs\n",
    "catalog1_lofar = Table.read(dr2_catalog1_file) #DR2\n",
    "catalog2_lofar = Table.read(ndr2_catalog2_file)\n",
    "catalog3_lofar = Table.read(dr2_catalog3_file) #DR2\n",
    "catalog4_lofar = Table.read(ndr2_catalog4_file)\n",
    "            \n",
    "\n",
    "##RM and RM err \n",
    "   \n",
    "#DR2   \n",
    "lof_rm=catalog1_lofar['rm']\n",
    "lof_rm_err=catalog1_lofar['rm_err']\n",
    "lof_rm3=catalog3_lofar['rm_1']\n",
    "lof_rm_err3=catalog3_lofar['rm_err_1']\n",
    "\n",
    " #NODR2\n",
    "lof_rm2=catalog2_lofar['phiPeakPIfit_rm2']\n",
    "lof_rm_err2 = catalog2_lofar['dPhiPeakPIfit_rm2']\n",
    "lof_rm4=catalog4_lofar['phiPeakPIfit_rm2_1']\n",
    "lof_rm_err4 = catalog4_lofar['dPhiPeakPIfit_rm2_1']\n",
    "\n",
    "##GRM and RRM\n",
    "\n",
    " #DR2\n",
    "lof_grm=catalog1_lofar['GRM2022_1deg']\n",
    "lof_grm_err=catalog1_lofar['GRMerr2022_1deg']\n",
    "lof_grm3=catalog3_lofar['GRM_1']\n",
    "lof_grm_err3=catalog3_lofar['GRMerr_1']\n",
    "\n",
    "lof_rrm=catalog1_lofar['RRM2022_1deg']\n",
    "lof_rrm3=catalog3_lofar['RRM2022_1deg_1']\n",
    "\n",
    " #NODR2\n",
    "lof_grm2=catalog2_lofar['GRM_wff']\n",
    "lof_grm_err2=catalog2_lofar['GRMerr_wff']\n",
    "lof_grm4=catalog4_lofar['GRM_wff']\n",
    "lof_grm_err4=catalog4_lofar['GRMerr_wff']\n",
    "\n",
    "lof_rrm2=catalog2_lofar['RRM']\n",
    "lof_rrm4=catalog4_lofar['RRM']\n",
    "\n",
    " #PDEG and DENSITY\n",
    "\n",
    " #DR2\n",
    "lof_pold=catalog1_lofar['fracpol']*100\n",
    "lof_pold3=catalog3_lofar['fracpol_1']*100\n",
    "\n",
    "lof_dens=catalog1_lofar['mean_density']\n",
    "lof_dens3=catalog3_lofar['density_TOT']\n",
    "\n",
    " #NODR2\n",
    "lof_pold2=catalog2_lofar['poldeg_spo']\n",
    "lof_pold4=catalog4_lofar['poldeg_spo_1']\n",
    "\n",
    "lof_dens2=catalog2_lofar['mean_density']\n",
    "lof_dens4=catalog4_lofar['density_TOT']\n",
    "\n",
    "\n",
    "rm_lof_combined = np.concatenate([lof_rm, lof_rm2, lof_rm3, lof_rm4])\n",
    "rrm_lof_combined = np.concatenate([lof_rrm, lof_rrm2, lof_rrm3, lof_rrm4])\n",
    "\n",
    "rm_err_lof_combined = np.concatenate([lof_rm_err, lof_rm_err2, lof_rm_err3, lof_rm_err4])\n",
    "grm_err_lof_combined = np.concatenate([lof_grm_err, lof_grm_err2, lof_grm_err3, lof_grm_err4 ])\n",
    "\n",
    "poldeg_lof_combined = np.concatenate([lof_pold, lof_pold2, lof_pold3, lof_pold4])\n",
    "rho_lof_combined = np.concatenate ([lof_dens, lof_dens2, lof_dens3, lof_dens4])\n",
    "\n",
    "\n",
    "####### DEFINE WEIGHTED FUNCTION \n",
    "\n",
    "def sigma_weight(mad_a, mad_b, mad_c, n_a, n_b, n_c):\n",
    "    \n",
    "    sigma_a=mad_a**2\n",
    "    sigma_b=mad_b**2\n",
    "    sigma_c=mad_c**2\n",
    "\n",
    "    delta_a=np.sqrt(2/n_a)*sigma_a\n",
    "    delta_b=np.sqrt(2/n_b)*sigma_b\n",
    "    delta_c=np.sqrt(2/n_c)*sigma_c\n",
    "\n",
    "    \n",
    "    sigma_tot=((sigma_a/delta_a**2) + (sigma_b/delta_b**2) + (sigma_c/delta_c**2))/(delta_a**(-2) + delta_b**(-2) + delta_c**(-2))\n",
    "    delta_tot=1/(np.sqrt(delta_a**(-2) + delta_b**(-2) + delta_c**(-2)))\n",
    "    \n",
    "    return sigma_tot, delta_tot\n",
    "\n",
    "\n",
    "#\n",
    "\n",
    "####### BINNING OF DATA\n",
    "custom_bin_edges = [-31, -29, -27.5, -26]\n",
    "\n",
    "#custom_bin_edges = [-31, -29.7, -27.6, -26]\n",
    "\n",
    "hist_nvss, bin_edges_nvss = np.histogram(np.log10(rho_nvss_combined), bins=custom_bin_edges, density=False)\n",
    "#bin_edges[20] = np.log10(np.max(rho_nvss_combined)+1e-26)\n",
    "\n",
    "digitized1 = np.digitize(np.log10(rho_nvss_combined), bin_edges_nvss)\n",
    "\n",
    "hist_lof, bin_edges_lof = np.histogram(np.log10(rho_lof_combined), bins=custom_bin_edges, density=False)\n",
    "#bin_edges[20] = np.log10(np.max(rho_lof_combined)+1e-26)\n",
    "\n",
    "digitized2 = np.digitize(np.log10(rho_lof_combined), bin_edges_lof)\n",
    "\n",
    "\n",
    "#print(hist)\n",
    "hist_combined = hist_nvss + hist_lof\n",
    "print(hist_combined)\n",
    "mean_density_values = [10**np.mean(bin_edges_nvss[i-1:i+1]) for i in range(1, len(bin_edges_nvss))]\n",
    "print(mean_density_values)\n",
    "\n",
    "# Loop through density bins\n",
    "variance_weight = []\n",
    "variance_weight_err = []\n",
    "\n",
    "for i in range(1, len(bin_edges_nvss)):\n",
    "    # Indices for points within the current density bin\n",
    "    indices_in_bin_nvss = (digitized1 == i)\n",
    "    indices_in_bin_lof = (digitized2 == i)\n",
    "\n",
    "    # Indices for points with poldeg > 5\n",
    "    indices_poldeg_high = indices_in_bin_nvss & (poldeg_nvss_combined > 5)\n",
    "    #print(i, np.where(indices_poldeg_high== True))\n",
    "\n",
    "    # Indices for points with poldeg < 5\n",
    "    indices_poldeg_low = indices_in_bin_nvss & (poldeg_nvss_combined < 5)\n",
    "    \n",
    "\n",
    "##### Calculate mad_std for NVSS poldeg > 5\n",
    "\n",
    "    ## RM ERR\n",
    "    med_stds_rm_err_a = np.median(rm_err_nvss_combined[indices_poldeg_high])\n",
    "    boot_med_stds_rm_err_a = bootstrap((rm_err_nvss_combined[indices_poldeg_high],), np.median,\n",
    "                random_state=None)\n",
    "    boot_err_med_stds_rm_err_a= boot_med_stds_rm_err_a.standard_error    \n",
    "    \n",
    "    ## GRM ERR\n",
    "    med_stds_grm_err_a = np.median(grm_err_nvss_combined[indices_poldeg_high])\n",
    "    boot_med_stds_grm_err_a = bootstrap((grm_err_nvss_combined[indices_poldeg_high],), np.median,\n",
    "                random_state=None)\n",
    "    boot_err_med_stds_grm_err_a = boot_med_stds_grm_err_a.standard_error\n",
    "    \n",
    "\n",
    "    ## RRM\n",
    "    mad_stds_rrm_a = mad_std(rrm_nvss_combined[indices_poldeg_high])\n",
    "    boot_mad_stds_rrm_a = bootstrap((rrm_nvss_combined[indices_poldeg_high],), np.var,\n",
    "                random_state=None)\n",
    "    boot_err_mad_stds_rrm_a= boot_mad_stds_rrm_a.standard_error\n",
    "\n",
    "    num_points_in_bin_a = np.sum(indices_poldeg_high)\n",
    "    print('Bin',i, 'num points in bin nvss high pol:', num_points_in_bin_a)\n",
    "\n",
    "    \n",
    "    mad_stds_rrm_real_a = np.sqrt(mad_stds_rrm_a**2-med_stds_grm_err_a**2-med_stds_rm_err_a**2)     \n",
    "    print('mad a', mad_stds_rrm_real_a)\n",
    "    print('errors a-->', boot_err_mad_stds_rrm_a)\n",
    "    \n",
    "##### Calculate mad_std for NVSS poldeg < 5\n",
    "    \n",
    "    ## RRM ERR\n",
    "    med_stds_rm_err_b = np.median(rm_err_nvss_combined[indices_poldeg_low])\n",
    "    boot_med_stds_rm_err_b = bootstrap((rm_err_nvss_combined[indices_poldeg_low],), np.median,\n",
    "                                       random_state=None)\n",
    "    boot_err_med_stds_rm_err_b= boot_med_stds_rm_err_b.standard_error\n",
    "    \n",
    "    ## GRM ERR\n",
    "    med_stds_grm_err_b = np.median(grm_err_nvss_combined[indices_poldeg_low])\n",
    "    boot_med_stds_grm_err_b = bootstrap((grm_err_nvss_combined[indices_poldeg_low],), np.median,\n",
    "                random_state=None)\n",
    "    boot_err_med_stds_grm_err_b = boot_med_stds_grm_err_b.standard_error\n",
    "    \n",
    "    ## RRM\n",
    "    mad_stds_rrm_b = mad_std(rrm_nvss_combined[indices_poldeg_low])\n",
    "    boot_mad_stds_rrm_b = bootstrap((rrm_nvss_combined[indices_poldeg_low],), np.var,\n",
    "                random_state=None)\n",
    "    boot_err_mad_stds_rrm_b= boot_mad_stds_rrm_b.standard_error\n",
    "\n",
    "    num_points_in_bin_b = np.sum(indices_poldeg_low)\n",
    "   \n",
    "    print('Bin',i, 'num points in bin nvss low pol:', num_points_in_bin_b)\n",
    "    \n",
    "    mad_stds_rrm_real_b = np.sqrt(mad_stds_rrm_b**2-med_stds_grm_err_b**2-med_stds_rm_err_b**2)\n",
    "    print('mad b', mad_stds_rrm_real_b)\n",
    " \n",
    "    print('errors -->', boot_err_mad_stds_rrm_b)\n",
    "    \n",
    "    \n",
    "##### Calculate mad_std for LOFAR points\n",
    " \n",
    "    ## RM ERR\n",
    "    med_stds_rm_err_c = np.median(rm_err_lof_combined[indices_in_bin_lof])\n",
    "    boot_med_stds_rm_err_c = bootstrap((rm_err_lof_combined[indices_in_bin_lof],), np.median,\n",
    "                                       random_state=None)\n",
    "    boot_err_med_stds_rm_err_c= boot_med_stds_rm_err_c.standard_error\n",
    "     \n",
    "    ## GRM ERR\n",
    "    med_stds_grm_err_c = np.median(grm_err_lof_combined[indices_in_bin_lof])\n",
    "    boot_med_stds_grm_err_c = bootstrap((grm_err_lof_combined[indices_in_bin_lof],), np.median,\n",
    "                                       random_state=None)\n",
    "    boot_err_med_stds_grm_err_c= boot_med_stds_grm_err_c.standard_error\n",
    "   \n",
    "    ## RRM\n",
    "    mad_stds_rrm_c = mad_std(rrm_lof_combined[indices_in_bin_lof])\n",
    "    boot_mad_stds_rrm_c = bootstrap((rrm_lof_combined[indices_in_bin_lof],), np.var,\n",
    "                                       random_state=None)\n",
    "    boot_err_mad_stds_rrm_c= boot_mad_stds_rrm_c.standard_error\n",
    "    num_points_in_bin_c = np.sum(indices_in_bin_lof)\n",
    "    print('Bin',i, 'num points in bin lofar:', num_points_in_bin_c)\n",
    "    \n",
    "    mad_stds_rrm_real_c = np.sqrt(mad_stds_rrm_c**2-med_stds_grm_err_c**2-med_stds_rm_err_c**2)\n",
    "    print('mad c', mad_stds_rrm_real_c)\n",
    "\n",
    "    print('errors -->', boot_err_mad_stds_rrm_c)\n",
    "\n",
    "    \n",
    "##### Calculate weighted variance \n",
    "#    sigma_tot, delta_tot = sigma_weight(mad_stds_rrm_real_a, mad_stds_rrm_real_b, \n",
    "#                                        mad_stds_rrm_real_c, boot_err_mad_stds_rrm_a,\n",
    "#                                        boot_err_mad_stds_rrm_b, boot_err_mad_stds_rrm_c)\n",
    "    sigma_tot, delta_tot = sigma_weight(mad_stds_rrm_real_a, mad_stds_rrm_real_b, \n",
    "                                        mad_stds_rrm_real_c, num_points_in_bin_a,\n",
    "                                        num_points_in_bin_b, num_points_in_bin_c)\n",
    "    \n",
    "    variance_weight.append(sigma_tot)\n",
    "    variance_weight_err.append(delta_tot)\n",
    "print('delta tot:', variance_weight_err)\n",
    "\n",
    "                      \n",
    "###### PLOT\n",
    "                      \n",
    "mad_variance=variance_weight\n",
    "mad_error=variance_weight_err\n",
    "    \n",
    "print(mad_variance, mad_error)\n",
    "\n",
    "fig = plt.figure(figsize=(8, 5))\n",
    "\n",
    "mad_error[2]=0.55\n",
    "\n",
    "plt.errorbar(mean_density_values, mad_variance, yerr=mad_error, fmt='o', zorder=10, color='black', elinewidth=0.7,\n",
    "             ms=4)\n",
    "\n",
    "#plt.title('LOFAR+NVSS Weighted RRM MAD vs. gas density')\n",
    "plt.rc('mathtext', fontset='dejavusans')\n",
    "plt.xlabel(r'$\\rho_{gas}$ [g cm$^{-3}$]')\n",
    "plt.ylabel(r'$\\sigma_{MAD}^{2_{RRM}}$ [rad$^2$ m$^{-4}$]')\n",
    "\n",
    "plt.xscale('log')\n",
    "\n",
    "min_y, max_y = plt.ylim()\n",
    "min_x, max_x = plt.xlim()\n",
    "\n",
    "plt.axvline(x=8*rho_c.value, color='red', linestyle='--',linewidth=1, label=r'$\\rho^{gas}_{200}$')\n",
    "#plt.axvline(x=500*rho_c.value, color='magenta', linestyle='--', label=r'$\\rho_{200}$')\n",
    "\n",
    "#plt.axvline(x=10**-29, color='blue', linestyle='--', label='Line 2')\n",
    "\n",
    "# Add color-filled regions between the vertical lines\n",
    "plt.fill_betweenx(y=[0, 10], x1=10**-31, x2=10**-28, color='orange', alpha=0.1, label='Voids')\n",
    "plt.fill_betweenx(y=[0, 10], x1=10**-29, x2=10**-27, color='blue', alpha=0.1, label='Filaments')\n",
    "plt.fill_betweenx(y=[0, 10], x1=10**-27.5, x2=10**-26, color='green', alpha=0.1, label='Nodes')\n",
    "\n",
    "plt.xlim([min_x, max_x])  \n",
    "plt.ylim([min_y, max_y])  \n",
    "\n",
    "\n",
    "plt.legend(loc='upper left')\n",
    "#plt.savefig('new_images/lofarxnvss_result_rrm_mad.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62455a85",
   "metadata": {},
   "source": [
    "I used a very similar code as up here to compute the resulting variance while shifting the binning edges. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e54fd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from astropy.table import Table\n",
    "from astropy.io import fits\n",
    "import fnmatch\n",
    "import matplotlib.pyplot as plt\n",
    "from astropy.stats import mad_std\n",
    "from astropy.cosmology import FlatLambdaCDM\n",
    "from scipy.stats import bootstrap\n",
    "%matplotlib inline\n",
    "\n",
    "base_directory = './catalogs/superclusters/overlap_corr/3D_overlapcorr/no_zeros/'  \n",
    "\n",
    "cosmo = FlatLambdaCDM(H0=70, Om0=0.3)\n",
    "rho_c=cosmo.critical_density(0.07)\n",
    "print(rho_c)\n",
    "\n",
    "# Define function to calculate weighted variance\n",
    "def sigma_weight(mad_a, mad_b, mad_c, n_a, n_b, n_c):\n",
    "    sigma_a=mad_a**2\n",
    "    sigma_b=mad_b**2\n",
    "    sigma_c=mad_c**2\n",
    "\n",
    "    delta_a=np.sqrt(2/n_a)*sigma_a\n",
    "    delta_b=np.sqrt(2/n_b)*sigma_b\n",
    "    delta_c=np.sqrt(2/n_c)*sigma_c\n",
    "\n",
    "    sigma_tot=((sigma_a/delta_a**2) + (sigma_b/delta_b**2) + (sigma_c/delta_c**2))/(delta_a**(-2) + delta_b**(-2) + delta_c**(-2))\n",
    "    delta_tot=1/(np.sqrt(delta_a**(-2) + delta_b**(-2) + delta_c**(-2)))\n",
    "    \n",
    "    return sigma_tot, delta_tot\n",
    "\n",
    "def weighted_avg_and_std(variances, weights):\n",
    "    \"\"\"\n",
    "    Returns the weighted average and standard error.\n",
    "    \"\"\"\n",
    "    weighted_avg = np.sum(weights * variances) / np.sum(weights)\n",
    "    \n",
    "    # Error on the weighted average\n",
    "    weighted_error = np.sqrt(1 / np.sum(weights))\n",
    "    \n",
    "    return weighted_avg, weighted_error\n",
    "\n",
    "################ NVSS ##################\n",
    "nvss_input_file_path1 = os.path.join(base_directory, 'nvss_cat_rm_allscc_nodup_final.fits')\n",
    "nvss_input_file_path2 = os.path.join(base_directory, 'overlap_nvss_final.fits')\n",
    "\n",
    "catalog1 = Table.read(nvss_input_file_path1)\n",
    "catalog2 = Table.read(nvss_input_file_path2)\n",
    "\n",
    "nvss_rm=catalog1['rm']\n",
    "nvss_rm_err=catalog1['rm_err_corr']\n",
    "nvss_rm2=catalog2['rm_1']\n",
    "nvss_rm_err2=catalog2['rm_err_corr']\n",
    "\n",
    "nvss_grm=catalog1['GRM_wff_v2022']\n",
    "nvss_grm_err=catalog1['GRMerr_wff_v2022']\n",
    "nvss_grm2=catalog2['GRM_wff_v2022']\n",
    "nvss_grm_err2=catalog2['GRMerr_wff_v2022']\n",
    "\n",
    "nvss_rrm=catalog1['RRM_v2022']\n",
    "nvss_rrm2=catalog2['RRM_v2022']\n",
    "\n",
    "nvss_pold=catalog1['fracpol']*100\n",
    "nvss_pold2=catalog2['fracpol_1']*100\n",
    "nvss_pold_e=catalog1['fracpol_err']*100\n",
    "nvss_pold_e2=catalog2['fracpol_err_1']*100\n",
    "\n",
    "nvss_dens=catalog1['mean_density']\n",
    "nvss_dens2=catalog2['density_TOT']\n",
    "\n",
    "rm_nvss_combined = np.concatenate([nvss_rm, nvss_rm2])\n",
    "rrm_nvss_combined = np.concatenate([nvss_rrm, nvss_rrm2])\n",
    "rm_err_nvss_combined = np.concatenate([nvss_rm_err, nvss_rm_err2])\n",
    "grm_err_nvss_combined = np.concatenate([nvss_grm_err, nvss_grm_err2])\n",
    "poldeg_nvss_combined = np.concatenate([nvss_pold, nvss_pold2])\n",
    "rho_nvss_combined = np.concatenate ([nvss_dens, nvss_dens2])\n",
    "\n",
    "###################### LOFAR #######################\n",
    "dr2_input_file_path1 = os.path.join(base_directory, 'lofar_dr2_rm_allscc_nodup.fits')\n",
    "ndr2_input_file_path2 = os.path.join(base_directory, 'lofar_nondr2_rm_allscc_nodup.fits')\n",
    "dr2_input_file_path3 = os.path.join(base_directory, 'DR2_overlap.fits')\n",
    "ndr2_input_file_path4 = os.path.join(base_directory, 'nonDR2_overlap.fits')\n",
    "\n",
    "catalog1_lofar = Table.read(dr2_input_file_path1)\n",
    "catalog2_lofar = Table.read(ndr2_input_file_path2)\n",
    "catalog3_lofar = Table.read(dr2_input_file_path3)\n",
    "catalog4_lofar = Table.read(ndr2_input_file_path4)\n",
    "\n",
    "lof_rm=catalog1_lofar['rm']\n",
    "lof_rm_err=catalog1_lofar['rm_err']\n",
    "lof_rm3=catalog3_lofar['rm_1']\n",
    "lof_rm_err3=catalog3_lofar['rm_err_1']\n",
    "lof_rm2=catalog2_lofar['phiPeakPIfit_rm2']\n",
    "lof_rm_err2 = catalog2_lofar['dPhiPeakPIfit_rm2']\n",
    "lof_rm4=catalog4_lofar['phiPeakPIfit_rm2_1']\n",
    "lof_rm_err4 = catalog4_lofar['dPhiPeakPIfit_rm2_1']\n",
    "\n",
    "lof_grm=catalog1_lofar['GRM2022_1deg']\n",
    "lof_grm_err=catalog1_lofar['GRMerr2022_1deg']\n",
    "lof_grm3=catalog3_lofar['GRM_1']\n",
    "lof_grm_err3=catalog3_lofar['GRMerr_1']\n",
    "lof_rrm=catalog1_lofar['RRM2022_1deg']\n",
    "lof_rrm3=catalog3_lofar['RRM2022_1deg_1']\n",
    "lof_grm2=catalog2_lofar['GRM_wff']\n",
    "lof_grm_err2=catalog2_lofar['GRMerr_wff']\n",
    "lof_grm4=catalog4_lofar['GRM_wff']\n",
    "lof_grm_err4=catalog4_lofar['GRMerr_wff']\n",
    "lof_rrm2=catalog2_lofar['RRM']\n",
    "lof_rrm4=catalog4_lofar['RRM']\n",
    "\n",
    "lof_pold=catalog1_lofar['fracpol']*100\n",
    "lof_pold3=catalog3_lofar['fracpol_1']*100\n",
    "lof_dens=catalog1_lofar['mean_density']\n",
    "lof_dens3=catalog3_lofar['density_TOT']\n",
    "lof_pold2=catalog2_lofar['poldeg_spo']\n",
    "lof_pold4=catalog4_lofar['poldeg_spo_1']\n",
    "lof_dens2=catalog2_lofar['mean_density']\n",
    "lof_dens4=catalog4_lofar['density_TOT']\n",
    "\n",
    "rm_lof_combined = np.concatenate([lof_rm, lof_rm2, lof_rm3, lof_rm4])\n",
    "rrm_lof_combined = np.concatenate([lof_rrm, lof_rrm2, lof_rrm3, lof_rrm4])\n",
    "rm_err_lof_combined = np.concatenate([lof_rm_err, lof_rm_err2, lof_rm_err3, lof_rm_err4])\n",
    "grm_err_lof_combined = np.concatenate([lof_grm_err, lof_grm_err2, lof_grm_err3, lof_grm_err4 ])\n",
    "poldeg_lof_combined = np.concatenate([lof_pold, lof_pold2, lof_pold3, lof_pold4])\n",
    "rho_lof_combined = np.concatenate ([lof_dens, lof_dens2, lof_dens3, lof_dens4])\n",
    "\n",
    "####### BINNING OF DATA\n",
    "custom_bin_edges = [-31, -29, -27.5, -26]\n",
    "num_shifts = 10\n",
    "shift_amount = 0.3  # Adjust the shift amount as needed\n",
    "\n",
    "all_mean_density_values = []\n",
    "all_mad_variances = []\n",
    "all_mad_errors = []\n",
    "\n",
    "# Generate shifted bin edges, including shift = 0\n",
    "shifts = np.linspace(-shift_amount, shift_amount, num_shifts)\n",
    "shifts = np.append(shifts, 0)  # Include the original bins\n",
    "\n",
    "# Generate shifted bin edges\n",
    "for shift in shifts:\n",
    "    shifted_bin_edges = [edge + shift for edge in custom_bin_edges]\n",
    "    print(shifted_bin_edges)\n",
    "    hist_nvss, bin_edges_nvss = np.histogram(np.log10(rho_nvss_combined), bins=shifted_bin_edges, density=False)\n",
    "    digitized1 = np.digitize(np.log10(rho_nvss_combined), bin_edges_nvss)\n",
    "    hist_lof, bin_edges_lof = np.histogram(np.log10(rho_lof_combined), bins=shifted_bin_edges, density=False)\n",
    "    digitized2 = np.digitize(np.log10(rho_lof_combined), bin_edges_lof)\n",
    "\n",
    "    mean_density_values = [10**np.mean(shifted_bin_edges[i-1:i+1]) for i in range(1, len(shifted_bin_edges))]\n",
    "    all_mean_density_values.append(mean_density_values)\n",
    "\n",
    "    variance_weight = []\n",
    "    variance_weight_err = []\n",
    "\n",
    "    for i in range(1, len(bin_edges_nvss)):\n",
    "        indices_in_bin_nvss = (digitized1 == i)\n",
    "        indices_in_bin_lof = (digitized2 == i)\n",
    "\n",
    "        indices_poldeg_high = indices_in_bin_nvss & (poldeg_nvss_combined > 5)\n",
    "        indices_poldeg_low = indices_in_bin_nvss & (poldeg_nvss_combined < 5)\n",
    "\n",
    "        ## RM ERR\n",
    "        med_stds_rm_err_a = np.median(rm_err_nvss_combined[indices_poldeg_high])\n",
    "        med_stds_rm_err_b = np.median(rm_err_nvss_combined[indices_poldeg_low])\n",
    "        med_stds_rm_err_c = np.median(rm_err_lof_combined[indices_in_bin_lof])\n",
    "\n",
    "        ## GRM ERR\n",
    "        med_stds_grm_err_a = np.median(grm_err_nvss_combined[indices_poldeg_high])\n",
    "        med_stds_grm_err_b = np.median(grm_err_nvss_combined[indices_poldeg_low])\n",
    "        med_stds_grm_err_c = np.median(grm_err_lof_combined[indices_in_bin_lof])\n",
    "\n",
    "        ## MAD\n",
    "        mad_stds_rrm_a = mad_std(rrm_nvss_combined[indices_poldeg_high])\n",
    "        #boot_mad_stds_rrm_a = bootstrap((rrm_nvss_combined[indices_poldeg_high],), mad_std, random_state=None).standard_error\n",
    "        mad_stds_rrm_real_a = np.sqrt(mad_stds_rrm_a**2 - med_stds_grm_err_a**2 - med_stds_rm_err_a**2)\n",
    "        num_points_in_bin_a = np.sum(indices_poldeg_high)\n",
    "        \n",
    "        mad_stds_rrm_b = mad_std(rrm_nvss_combined[indices_poldeg_low])\n",
    "        #boot_mad_stds_rrm_b = bootstrap((rrm_nvss_combined[indices_poldeg_low],), mad_std, random_state=None).standard_error\n",
    "        mad_stds_rrm_real_b = np.sqrt(mad_stds_rrm_b**2 - med_stds_grm_err_b**2 - med_stds_rm_err_b**2)\n",
    "        num_points_in_bin_b = np.sum(indices_poldeg_low)\n",
    "        \n",
    "        mad_stds_rrm_c = mad_std(rrm_lof_combined[indices_in_bin_lof])\n",
    "        #boot_mad_stds_rrm_c = bootstrap((rrm_lof_combined[indices_in_bin_lof],), mad_std, random_state=None).standard_error\n",
    "        mad_stds_rrm_real_c = np.sqrt(mad_stds_rrm_c**2 - med_stds_grm_err_c**2 - med_stds_rm_err_c**2)\n",
    "        num_points_in_bin_c = np.sum(indices_in_bin_lof)\n",
    "        \n",
    "        ## Calculate weighted variance \n",
    "#        sigma_tot, delta_tot = sigma_weight(mad_stds_rrm_real_a, mad_stds_rrm_real_b, mad_stds_rrm_real_c,\n",
    "#                                            boot_mad_stds_rrm_a, boot_mad_stds_rrm_b, boot_mad_stds_rrm_c)\n",
    "\n",
    "        sigma_tot, delta_tot = sigma_weight(mad_stds_rrm_real_a, mad_stds_rrm_real_b, mad_stds_rrm_real_c,\n",
    "                                            num_points_in_bin_a,\n",
    "                                            num_points_in_bin_b, \n",
    "                                            num_points_in_bin_c)\n",
    "\n",
    "        variance_weight.append(sigma_tot)\n",
    "        variance_weight_err.append(delta_tot)\n",
    "\n",
    "    all_mad_variances.append(variance_weight)\n",
    "    all_mad_errors.append(variance_weight_err)\n",
    "\n",
    "outerbin=[]\n",
    "for i in all_mad_variances:\n",
    "    outerbin.append(i[0])\n",
    "\n",
    "print(np.mean(outerbin))\n",
    "print(np.std(outerbin))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44113e2",
   "metadata": {},
   "source": [
    "###  Step 3: Bayesian Modeling of Rotation Measures with `emcee`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12709a6b",
   "metadata": {},
   "source": [
    "We model the RRM variance with a model from Murgia+2009. Parameters are density, magnetic field, path length, magnetic field reversal scale. We use emcee to run the MCMC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d381f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import emcee\n",
    "\n",
    "\n",
    "#Define prior distribution for parameters \n",
    "\n",
    "def log_prior(p):\n",
    "    #B, xx = p\n",
    "    B, L, Lambda = p\n",
    "    if 0 < B < 2 and 5e6 < L < 70e6 and 5e3 < Lambda < 500e3:\n",
    "        return 0.0\n",
    "    return -np.inf\n",
    "\n",
    "#Define likelihood function\n",
    "\n",
    "def log_likelihood(p, ne, sigma_RM_obs, sigma_RM_obs_err):\n",
    "    #B, xx = p\n",
    "    B, L, Lambda = p\n",
    "    sigma_RM_model = (0.812*ne*B*np.sqrt(L*Lambda))**2\n",
    "    delta_sigma = sigma_RM_obs_err\n",
    "    return -0.5 * np.sum((sigma_RM_model - sigma_RM_obs) ** 2 / delta_sigma**2 +np.log(2*np.pi*delta_sigma**2))\n",
    "\n",
    "#Define the full log-probability : posterior = likelihood x prior (flat prior 0 so +)\n",
    "\n",
    "def log_probability(p, ne, sigma_RM_obs, sigma_RM_obs_err):\n",
    "    lp = log_prior(p)\n",
    "    if not np.isfinite(lp):\n",
    "        return -np.inf\n",
    "    return lp + log_likelihood(p, ne, sigma_RM_obs, sigma_RM_obs_err)\n",
    "#sampler.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f070b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observed data\n",
    "\n",
    "ne=np.array([10**-6, 5.6*10**(-5)]) #number density\n",
    "\n",
    "sigma_RM_obs=np.array([4.571330365219012, 7.208651609225383])\n",
    "sigma_RM_obs_err=np.array([0.35240179799195087, 0.312821129639818])\n",
    "\n",
    "\n",
    "# Initial guess for parameter values\n",
    "p_initial_guess = [1, 10e6, 500e3] #B, L, Lambda\n",
    "\n",
    "# Number of walkers\n",
    "nwalkers = 32 \n",
    "\n",
    "# Scatter the walkers around the initial guess\n",
    "p_initial_guess_scattered = np.array(p_initial_guess) + 1e-3 * np.random.randn(nwalkers, len(p_initial_guess))\n",
    "\n",
    "#sampler.reset()\n",
    "\n",
    "# Initialize the sampler with the initial position of the walkers\n",
    "sampler = emcee.EnsembleSampler(nwalkers, len(p_initial_guess), log_probability, args=(ne, sigma_RM_obs, sigma_RM_obs_err))\n",
    "\n",
    "# Run the MCMC chain\n",
    "n_steps = 10000\n",
    "sampler.run_mcmc(p_initial_guess_scattered, n_steps, progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c577c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, figsize=(10, 7), sharex=True)\n",
    "samples = sampler.get_chain()\n",
    "labels = [r\"B$_{//}$ [$\\mu$G]\", \"L [pc]\", r\"$\\Lambda_c$ [pc]\"]\n",
    "for i in range(len(p_initial_guess)):\n",
    "    ax = axes[i]\n",
    "    ax.plot(samples[:, :, i], \"k\", alpha=0.3)\n",
    "    ax.set_xlim(0, len(samples))\n",
    "    ax.set_ylabel(labels[i])\n",
    "    #ax.yaxis.set_label_coords(-0.1, 0.5)\n",
    "\n",
    "axes[-1].set_xlabel(\"step number\");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d797fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is good practice to remove the first 500 burn in steps\n",
    "\n",
    "flat_samples = sampler.get_chain(discard=500, thin=15, flat=True)\n",
    "print(flat_samples.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9bbcf1",
   "metadata": {},
   "source": [
    "### Step 4: Posterior Analysis and Visualization with `corner`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef45f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import corner\n",
    "%matplotlib inline\n",
    "\n",
    "ranges = [(0, 0.12),(5e6, 70e6), (5e3, 500e3)]\n",
    "hist_kwargs = {\"color\": \"dodgerblue\", \"edgecolor\": \"black\", \"linewidth\": 1}\n",
    "\n",
    "fig = corner.corner(\n",
    "    flat_samples, labels=labels,quantiles=[0.0275, 0.5, 0.975], levels=(0.68, 0.95), fill_contours=True, color='dodgerblue', \n",
    "                    contour_kwargs={\"colors\":'black',\"linewidths\": 0.8}, bins=50, range=ranges, hist_kwargs={\"color\": \"dodgerblue\", \"edgecolor\": \"black\", \"linewidth\": 0.8}\n",
    ");\n",
    "#fig.savefig('new_images/corner.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ebd149",
   "metadata": {},
   "source": [
    "To focus on the magnetic field distribution only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab871a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import trim_mean\n",
    "\n",
    "chain = sampler.get_chain(discard=500)[:, :, 0].T\n",
    "\n",
    "flat_chain = chain.flatten()\n",
    "median_value = np.median(flat_chain)\n",
    "trimmed_mean_value = trim_mean(flat_chain, proportiontocut=0.2)\n",
    "\n",
    "hist, bin_edges = np.histogram(flat_chain, bins=100)\n",
    "mode_index = np.argmax(hist)\n",
    "mode_value = (bin_edges[mode_index] + bin_edges[mode_index + 1]) / 2\n",
    "\n",
    "# 95% confidence interval using percentiles\n",
    "lower_bound = np.percentile(flat_chain, 2.5)\n",
    "upper_bound = np.percentile(flat_chain, 97.5)\n",
    "\n",
    "print(f\"Median: {median_value:.2f}\")\n",
    "print(f\"20% Trimmed Mean: {trimmed_mean_value:.2f}\")\n",
    "print(f\"Mode (approx): {mode_value:.2f}\")\n",
    "print(f\"95% Confidence Interval: [{lower_bound:.2f}, {upper_bound:.2f}]\")\n",
    "\n",
    "plt.hist(flat_chain, bins=50, alpha=0.5, histtype='stepfilled', color='dodgerblue', edgecolor='black')\n",
    "\n",
    "plt.axvline(mode_value, color='deeppink', linestyle='dashed', linewidth=1, label=f'Mode')\n",
    "plt.axvline(lower_bound, color='navy', linestyle='dashed', linewidth=1, label=f'95% confidence')\n",
    "plt.axvline(upper_bound, color='navy', linestyle='dashed', linewidth=1)# label=f'95% CI Upper: {upper_bound:.2f}')\n",
    "plt.fill_betweenx(y=[0, 23000], x1=0, x2=0.005, color='purple', alpha=0.2, label='Adiabatic compression')\n",
    "\n",
    "# Add legend and labels\n",
    "plt.legend(fontsize='small')\n",
    "plt.xlim(0, 0.13)\n",
    "plt.ylim(0, 20000)\n",
    "\n",
    "plt.yticks([])\n",
    "\n",
    "plt.xlabel(r'B$_{//}$[$\\mu$G]')\n",
    "#plt.ylabel('Frequency')\n",
    "plt.title('Marginalised 1D posterior distribution')\n",
    "\n",
    "# Show the plot\n",
    "#plt.savefig('new_images/B_hist.pdf')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912c39cd",
   "metadata": {},
   "source": [
    "You can fix one of the parameters to explore the results. We fixed L=70 Mpc, the maximum path length, to minimize B. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541cd6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import emcee\n",
    "\n",
    "# Fixed L value\n",
    "L_fixed = 70e6  # You can change this to the desired fixed value\n",
    "\n",
    "def log_prior(p):\n",
    "    B, Lambda = p\n",
    "    if 0 < B < 2 and 5e3 < Lambda < 500e3:\n",
    "        return 0.0\n",
    "    return -np.inf\n",
    "\n",
    "def log_likelihood(p, ne, sigma_RM_obs, sigma_RM_obs_err):\n",
    "    B, Lambda = p\n",
    "    sigma_RM_model = (0.812 * ne * B * np.sqrt(L_fixed * Lambda))**2\n",
    "    delta_sigma = sigma_RM_obs_err\n",
    "    return -0.5 * np.sum((sigma_RM_model - sigma_RM_obs) ** 2 / delta_sigma ** 2 + np.log(2 * np.pi * delta_sigma ** 2))\n",
    "\n",
    "def log_probability(p, ne, sigma_RM_obs, sigma_RM_obs_err):\n",
    "    lp = log_prior(p)\n",
    "    if not np.isfinite(lp):\n",
    "        return -np.inf\n",
    "    return lp + log_likelihood(p, ne, sigma_RM_obs, sigma_RM_obs_err)\n",
    "\n",
    "# Observed data\n",
    "ne = np.array([10**-6, 5.6*10**(-5)])\n",
    "sigma_RM_obs=np.array([4.571330365219012, 7.208651609225383])\n",
    "sigma_RM_obs_err=np.array([0.35240179799195087, 0.312821129639818])\n",
    "\n",
    "p_initial_guess = [1, 500e3]  # B, Lambda\n",
    "\n",
    "nwalkers = 32 \n",
    "\n",
    "p_initial_guess_scattered = np.array(p_initial_guess) + 1e-3 * np.random.randn(nwalkers, len(p_initial_guess))\n",
    "\n",
    "sampler = emcee.EnsembleSampler(nwalkers, len(p_initial_guess), log_probability, args=(ne, sigma_RM_obs, sigma_RM_obs_err))\n",
    "\n",
    "# Run the MCMC chain\n",
    "n_steps = 10000\n",
    "sampler.run_mcmc(p_initial_guess_scattered, n_steps, progress=True)\n",
    "\n",
    "# remove burn-in\n",
    "flat_samples = sampler.get_chain(discard=1000, thin=15, flat=True)\n",
    "\n",
    "# Plot\n",
    "import corner\n",
    "\n",
    "fig = corner.corner(flat_samples, labels=[\"B\", \"Lambda\"], truths=[None, None])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96233d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import corner\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "\n",
    "fig = corner.corner(flat_samples, labels=[\"B\", \"Lambda\"], plot_density=False, \n",
    "                    levels=(0.68,0.95), fill_contours=True, color='dodgerblue', \n",
    "                    contour_kwargs={\"colors\":'black',\"linewidths\": 1}, bins=50)\n",
    "fig.set_size_inches(10, 8)\n",
    "\n",
    "x_limits = [0, 0.1]      # Limits for B\n",
    "y_limits = [10000, 500000]  # Limits for Lambda\n",
    "\n",
    "axes = np.array(fig.axes).reshape((2, 2))\n",
    "\n",
    "# limits for the B vs. Lambda plot\n",
    "axes[1, 0].set_xlim(x_limits)\n",
    "axes[1, 0].set_ylim(y_limits)\n",
    "axes[1, 0].set_ylabel(r'$\\Lambda_c$ [kpc]')\n",
    "axes[1, 0].set_xlabel(r'$B_{||}$ [$\\mu$G]')\n",
    "axes[1, 0].axvline(0.005, color='red', linestyle='--', linewidth=1.5)\n",
    "\n",
    "axes[1, 0].set_title(r'$L=70$ Mpc')\n",
    "\n",
    "def rescale_ticks(x, pos):\n",
    "    return f'{x / 1000:.0f}'\n",
    "axes[1, 0].yaxis.set_major_formatter(FuncFormatter(rescale_ticks))\n",
    "\n",
    "\n",
    "# Remove the 1D histograms\n",
    "for ax in axes[0, :]:\n",
    "    ax.set_visible(False)\n",
    "for ax in axes[:, 1]:\n",
    "    ax.set_visible(False)\n",
    "\n",
    "#plt.savefig('images/test_L.pdf')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b71aa86",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In this notebook we performed a **Bayesian statistical analysis** of magnetic fields in galaxy superclusters.  \n",
    "By combining RM-Synthesis with MCMC sampling using `emcee`, we obtained:\n",
    "\n",
    "- Robust posterior distributions of rotation measures (RMs)\n",
    "- Credible intervals that capture astrophysical uncertainties\n",
    "- Visual diagnostics of parameter correlations via `corner` plots\n",
    "\n",
    "This approach highlights the value of **Bayesian methods in astrophysics**:  \n",
    "they provide transparent uncertainty quantification and allow principled comparisons of models.  \n",
    "\n",
    "Beyond the astrophysical context, this project demonstrates practical skills in:\n",
    "- **MCMC sampling** with `emcee`\n",
    "- **Statistical modeling** with `scipy`\n",
    "- **Posterior visualization** with `corner`\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
